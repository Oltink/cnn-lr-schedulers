# ðŸ§  CNN + Learning Rate Schedulers (FashionMNIST)

This project explores how different **Learning Rate Schedulers** affect the training of a CNN on the **FashionMNIST** dataset using **PyTorch**.

## ðŸ“Œ Task (from coursework)
The goal is to apply the following LR schedulers to a simplified CNN:
- `ExponentialLR`
- `MultiStepLR`
- `CosineAnnealingLR`

and compare results to a baseline model without scheduling.

## ðŸ§± Model: Reduced CNN
To simulate a lighter model (as per Assignment 4), one convolution block was removed.

## ðŸ“Š Results (Test Accuracy)

| Scheduler         | Accuracy |
|------------------|----------|
| No Scheduler      | 91.17%   |
| ExponentialLR     | 91.96%   |
| MultiStepLR       | 91.88%   |
| CosineAnnealingLR | **91.93%** âœ… |

## ðŸ“ˆ Training Loss Comparison
![Training Loss](imgs/loss_plot_comparison.png)

## ðŸ§  Conclusion
> All schedulers slightly improved training. **CosineAnnealingLR** achieved the best accuracy, likely due to smoother LR decay that avoids overfitting in early epochs.

## ðŸ§° Technologies
- Python
- PyTorch
- Matplotlib
- FashionMNIST (torchvision)

## ðŸš€ Run this notebook
```bash
pip install -r requirements.txt
jupyter notebook
```

## ðŸ“· Sample Outputs

**No Scheduler**
![Baseline](imgs/loss_baseline.png)

**With ExponentialLR**
![ExponentialLR](imgs/loss_exponentiallr.png)

**With MultiStepLR**
![MultiStepLR](imgs/loss_multisteplr.png)

**With CosineAnnealingLR**
![CosineAnnealingLR](imgs/loss_cosineannealinglr.png)
